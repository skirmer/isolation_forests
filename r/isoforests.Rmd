---
title: "Branching Out Into Isolation Forests"
subtitle: R-Ladies Dallas
author: "Stephanie Kirmer <BR> [www.stephaniekirmer.com](http://www.stephaniekirmer.com) <BR> @[data_stephanie](http://www.twitter.com/data_stephanie)"
date: "December 7, 2020"
output:
  xaringan::moon_reader:
    css: [middlebury-fonts, default]
    nature:
      highlightLines: true
---

```{r, include = FALSE}
library(gapminder)
library(ggplot2)

library(xaringan)
library(xaringanExtra)
xaringanExtra::use_webcam()
knitr::opts_chunk$set(echo = FALSE)
xaringanExtra::use_panelset()

library(isotree)
library(plotly)
library(patchwork)
library(fastshap)

```

# Follow Along! 

https://github.com/skirmer/isolation_forests

---
# Introduction

Isolation forests are a method using tree-based decisionmaking to separate observations instead of grouping them. You might visualize this in tree form:


<img src="../IsolationForest1.png" alt="diagram1" width="600"/>

---
# Introduction

If you prefer to think about the points in two dimensional space, you can also use something like this:

![](../2d_diagram_b.png)


Here you can see that a highly anomalous observation is easily separated from the bulk of the sample, while a non-anomalous one requires many more steps to isolate.

---
# Getting Started

Today we are going to implement this modeling approach using a sample of data from Spotify- song characteristics.

We'll be using these libraries:

* **modeling**: isotree, fastshap   
* **visuals**: ggplot2, plotly, patchwork   

---
# Load Data

Our dataset: Spotify Tracks (via Kaggle)

https://www.kaggle.com/yamaerenay/spotify-dataset-19212020-160k-tracks?select=data.csv

## Track Characteristics

```{r}
dataset = read.csv("/Users/skirmer/repos/isolation_forests/data.csv")
colnames(dataset)
```

---
# Looking at Examples

.panelset[
.panel[.panel-name[Instrumental]

```{r , echo=TRUE}
knitr::kable(head(dataset[dataset$instrumentalness > .94, c("artists", "name", "year")], 5))
```

]
.panel[.panel-name[Speechy]

```{r , echo=TRUE}
set.seed(426)
speechy = dataset[dataset$speechiness > .9 & dataset$year > 1965,
                  c("artists", "name", "year")]
knitr::kable(speechy[sample(nrow(speechy), 3), ])

```

]
.panel[.panel-name[Loud]

```{r, echo=TRUE}
set.seed(400)
loud = dataset[dataset$loudness > .85, c("artists", "name", "year")]
knitr::kable(loud[sample(nrow(loud), 3), ])

```

]]

---
# Feature Engineering

* Bin the years
* Cut off songs pre-1960


```{r, echo=FALSE}
dataset = dataset[dataset$year > 1960,]

b <- c(-Inf, 1970, 1980, 1990, 2000, 2010, Inf)
names <- c("60s", "70s", "80s", "90s", "00s", "10s to present")
dataset$year_bin <- cut(dataset$year, breaks = b, labels = names)

knitr::kable(table(dataset$year_bin))
```

```{r, echo=TRUE, include = FALSE}

features = c('acousticness', 'explicit', 'duration_ms','instrumentalness', 'key',
            'liveness', 'loudness', 'mode', 'popularity','valence',
            'speechiness', 'tempo', 'energy', 'danceability')

# Train Test Split
train_ind <- sample(nrow(dataset), size = nrow(dataset)*.7)
training_set <- dataset[train_ind, c('artists','name','year', 'year_bin', features)]
test_set <- dataset[-train_ind, c('artists','name','year', 'year_bin', features)]

```

---
# Function Syntax

We don't need to set any outcome or dependent variable because that is not the objective of this algorithm.

.panelset[
.panel[.panel-name[Calling Function]

```{r, echo=TRUE}
iso_ext = isolation.forest(
  training_set[, features], 
  ndim=1,
  ntrees=100,
  max_depth = 6,
  prob_pick_pooled_gain=0,
  prob_pick_avg_gain=0,
  output_score = FALSE)

Z1 <- predict(iso_ext, training_set)
Z2 <- predict(iso_ext, test_set)

training_set$scores <- Z1
test_set$scores <- Z2
```
]


.panel[.panel-name[Fitting Options]

You also have a few options in how you fit the model. You can:
* `output_score = TRUE`
  - return "outlierness" scores for training set  
*  `output_dist = TRUE`
  - return pairwise distance between points (eg, difference degree between any two- it takes time to run)
* return a model object to be used for predicting on new data (default) 

<img src="../tree_sample.png" alt="diagram2" width="400"/>

]

.panel[.panel-name[Model Summary Data]

These are examples of the available output from returning a model object.

```{r, echo=TRUE}
summary(iso_ext)
```
]]


---
# Model Tuning


Some of the hyperparameters will be very familiar from other kinds of tree-based models. These are others that might be worth tuning for your modeling.

* `prob_pick_pooled_gain=0,`: higher value fits closer to training set, but risk of overfitting
* `prob_pick_avg_gain=0,`: higher value is more likely to set outlier bounds outside training set range (less overfitting, but worse training performance)

When a split is created in tree, it's random by default. These arguments change that, to increase probability of the split giving largest average or pooled gain. If you pass 1 to either, that creates a deterministic tree.

---
# "Ghosting" Issue

One more hyperparameter to look at: `ndim` indicates number of columns to combine to produce a split

Having multiple clusters of non-anomaly points can create problems when using isolation forests, as shown here. 
This is the issue that Extended Isolation Forest (ndim > 1) is meant to remedy.

```{r, fig.width=9,dpi = 400, fig.height=4, out.width=700, echo=FALSE, message=FALSE, error=FALSE, warning=FALSE}
source("ghosting_poc.R")

p1 + p2 + p3 + plot_layout(ncol=3)
```


---

# Feature Importance
```{r, echo=FALSE, warning=F, error=F, message=F, dpi = 400, fig.height=4}

shapley_values <- fastshap::explain(iso_ext, X=training_set[, features], pred_wrapper = predict)
autoplot(shapley_values)+theme_bw()+labs(title="Feature Importances")
```

---
# Peeking at Results


.panelset[
.panel[.panel-name[Table of Tracks]

```{r}

knitr::kable(tail(training_set[, c('artists','name','year','scores')]))

```
]
.panel[.panel-name[Score Distribution]

```{r, echo=FALSE, fig.width=9,dpi = 400, fig.height=5, out.width=600}

ggplot(training_set, aes(x=scores))+
  theme_bw()+
  geom_density()

```

]
.panel[.panel-name[Scatterplot (Training)]


```{r, echo=FALSE, fig.width=9,dpi = 400, fig.height=5, out.width=650}
training_set$anomaly = ifelse(training_set$scores > .52, "Anomaly", "Normal")
ggplot(training_set, aes(x=tempo, y=speechiness, 
                         group = anomaly, color = anomaly))+
  theme_bw()+
  geom_point(alpha = .75)+
  labs(title="Training Sample Score")
```

]
.panel[.panel-name[Scatterplot (Test)]


```{r, echo=FALSE, fig.width=9,dpi = 400, fig.height=5, out.width=600}
test_set$anomaly = ifelse(test_set$scores > .52, "Anomaly", "Normal")
ggplot(test_set, aes(x=tempo, y=speechiness, 
                     group = anomaly, color = anomaly))+
  theme_bw()+
  geom_point(alpha = .75)+
  labs(title="Test Sample Score")
```
]]

---
# PCA



.panelset[
.panel[.panel-name[Component Choices]

```{r, echo=FALSE, fig.width=9,dpi = 400, fig.height=5, out.width=650}
trainingpca <- prcomp(training_set[, features], scale. = T)

std_dev <- trainingpca$sdev
pr_var <- std_dev^2
prop_varex <- pr_var/sum(pr_var)

plot(prop_varex, xlab = "Principal Component",
             ylab = "Proportion of Variance Explained",
             type = "b")

trainingpca = data.frame(training_set[, c("artists", "name", "year", "scores", "anomaly")], trainingpca$x)

```

]
.panel[.panel-name[3D Rendering]


```{r, echo=FALSE}
tpca = trainingpca[, c(1:8)]

m <- list(
  l = 20,
  r = 20,
  b = 0,
  t = 0,
  pad = 20
)
```

```{r, echo=FALSE, dpi = 400, out.height = 300, out.width=650, warning=F, error=F, message=F}

tpca$anomaly <- as.factor(tpca$anomaly)

fig <- plot_ly(tpca, x = ~PC2, y = ~PC3, z = ~PC1, color = ~anomaly, colors = c('#BF382A', '#0C4B8E'), width = 600, height = 350)
fig <- fig %>% add_markers(size= 2)
fig <- fig %>% layout(scene = list(xaxis = list(title = 'PC2'),
                     yaxis = list(title = 'PC3'),
                     zaxis = list(title = 'PC1')))

fig %>% layout(autosize = F, margin = m)
```
]


.panel[.panel-name[3D Outliers]


```{r, echo=FALSE}
tpca = trainingpca[, c(1:8)]

m <- list(
  l = 20,
  r = 20,
  b = 0,
  t = 0,
  pad = 20
)
```

```{r, echo=FALSE, dpi = 400, out.height = 300, out.width=650, warning=F, error=F, message=F}

tpca$anomaly <- as.factor(tpca$anomaly)
tpca_anom = tpca[tpca$anomaly == 'Anomaly',]
tpca_anom$labeltext = paste(tpca_anom$name, tpca_anom$artists, tpca_anom$year, sep = "-")

labelers = seq(0, nrow(tpca_anom), length.out = 10)

fig <- plot_ly(tpca_anom, 
               x = ~PC2, y = ~PC3, z = ~PC1, 
               color = ~anomaly, 
               colors = c('#BF382A', '#0C4B8E'), 
               alpha = .5,
               width = 600, height = 350)
fig <- fig %>% add_markers(size= 2)
#fig <- fig %>% add_text(data = tpca_anom[labelers, ], text = tpca_anom[labelers, 'name'], size = .1, alpha = 1)
fig <- fig %>% layout(scene = list(xaxis = list(title = 'PC2'),
                     yaxis = list(title = 'PC3'),
                     zaxis = list(title = 'PC1')))

fig %>% layout(autosize = F, margin = m)
```
]


.panel[.panel-name[3D Outliers Named]


```{r, echo=FALSE}
tpca = trainingpca[, c(1:8)]

m <- list(
  l = 20,
  r = 20,
  b = 0,
  t = 0,
  pad = 20
)
```

```{r, echo=FALSE, dpi = 400, out.height = 300, out.width=650, warning=F, error=F, message=F}

tpca$anomaly <- as.factor(tpca$anomaly)
tpca_anom = tpca[tpca$anomaly == 'Anomaly',]
tpca_anom$labeltext = paste(tpca_anom$name, tpca_anom$artists, tpca_anom$year, sep = "-")

labelers = seq(0, nrow(tpca_anom), length.out = 15)

fig <- plot_ly(tpca_anom, 
               x = ~PC2, y = ~PC3, z = ~PC1, 
               color = ~anomaly, 
               width = 600, height = 350)
fig <- fig %>% add_markers(size= 1, opacity = .2, colors = c('#BF382A', '#0C4B8E'))
fig <- fig %>% add_markers(data = tpca_anom[labelers, ], x = ~PC2, y = ~PC3, z = ~PC1, alpha = 1, size = 2)
fig <- fig %>% add_text(data = tpca_anom[labelers, ], text = tpca_anom[labelers, 'name'], size = .1, alpha = 1, textposition = "top right")
fig <- fig %>% layout(scene = list(xaxis = list(title = 'PC2'),
                     yaxis = list(title = 'PC3'),
                     zaxis = list(title = 'PC1')))

fig %>% layout(autosize = F, margin = m)
```
]

]

---
# Other Exploration


.panelset[
.panel[.panel-name[Decade Proportions]

```{r, fig.width=9,dpi = 400, fig.height=5, out.width=700}

ggplot(data = training_set, aes(x=as.factor(year_bin),fill = as.factor(anomaly)))+
  theme_bw()+
  geom_bar(stat = "count") + 
  stat_count(geom = "label", size = 3.5, aes(label = ..count..), position=position_stack(vjust=0.5))+
  labs(title="Anomalousness by Decade", x="Decade", y="Records", fill = "Label")
```

]
.panel[.panel-name[Score Density]

```{r, fig.width=9,dpi = 400, fig.height=5, out.width=700}
ggplot(training_set, aes(x=scores, y=as.factor(anomaly))) + theme_bw() + geom_bin2d() + labs(title = 'Density of Scores', x="Scores", y="Anomaly Status")
```
]]

---
# Further Links/Reference

https://ggplot2.tidyverse.org/  
https://plotly.com/r/3d-scatter-plots/   
https://github.com/david-cortes/isotree    
https://github.com/david-cortes/outliertree (A different but related algorithm for tree based outlier identification)  
https://towardsdatascience.com/outlier-detection-with-extended-isolation-forest-1e248a3fe97b (More on the ghosting problem)   
https://arxiv.org/pdf/1811.02141.pdf

---
# Thank you!

[www.stephaniekirmer.com](http://www.stephaniekirmer.com) | @[data_stephanie](http://www.twitter.com/data_stephanie) |  [saturncloud.io](http://saturncloud.io)  
